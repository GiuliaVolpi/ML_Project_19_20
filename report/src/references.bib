@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@book{intel-alt,
    title    = {Intel Math Kernel Library. Reference Manual},
    publisher= {Intel Corporation},
    address  = {Santa Clara, USA},
    note     = {ISBN 630813-054US},
    year     = {2009},
}


@ARTICLE{2019arXiv190710121V,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E.  and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0--Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {arXiv e-prints},
         year = "2019",
        month = "Jul",
          eid = {arXiv:1907.10121},
        pages = {arXiv:1907.10121},
archivePrefix = {arXiv},
       eprint = {1907.10121},
 primaryClass = {cs.MS},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@Inbook{Prechelt2012,
author="Prechelt, Lutz",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_5",
url="https://doi.org/10.1007/978-3-642-35289-8_5"
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{nakerst2020gradient,
    title={Gradient descent with momentum --- to accelerate or to super-accelerate?},
    author={Goran Nakerst and John Brennan and Masudul Haque},
    year={2020},
    eprint={2001.06472},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Polyak1964,
author = {Polyak, Boris},
year = {1964},
month = {12},
pages = {1-17},
title = {Some methods of speeding up the convergence of iteration methods},
volume = {4},
journal = {Ussr Computational Mathematics and Mathematical Physics},
doi = {10.1016/0041-5553(64)90137-5}
}

@article{sutskever2013,
author = {Sutskever, I. and Martens, J. and Dahl, G. and Hinton, G.},
year = {2013},
month = {01},
pages = {1139-1147},
title = {On the importance of initialization and momentum in deep learning},
journal = {30th International Conference on Machine Learning, ICML 2013}
}

@INPROCEEDINGS{Krizhevsky_imagenetclassification,
    author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    title = {Imagenet classification with deep convolutional neural networks},
    booktitle = {Advances in Neural Information Processing Systems},
    year = {},
    pages = {2012}
}

@INPROCEEDINGS{Glorot10understandingthe,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATSâ€™10). Society for Artificial Intelligence and Statistics},
    year = {2010}
}


@book{haykin2009neural,
  abstract = {Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists.
 
Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/
 
Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.},
  added-at = {2017-03-18T17:31:57.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Haykin, Simon S.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5015812328aaeccd73d8b03a7e36831/vngudivada},
  edition = {Third},
  interhash = {4cef19efafc52ae42607f9832a205214},
  intrahash = {e5015812328aaeccd73d8b03a7e36831},
  keywords = {Book Learning NeuralNetwork},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks and learning machines},
  year = 2009
}

